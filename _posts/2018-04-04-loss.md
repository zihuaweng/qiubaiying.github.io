---
layout:     post
title:      损失函数汇总及tensorflow实践
subtitle:   
date:       2018-04-01
author:     zihuaweng
header-img: img/post_header.jpg
catalog: true
tags:
    - loss function
    - tensorflow
---

## 激活函数

sigmoid和softmax函数的存在是将计算的logit转化为概率p。

概率p的logit是\\(L=\ln\frac{p}{1-p} \\)。其中\\(\frac{p}{1-p}\\)是odd(两者比率)，logit反过来就是\\(p= \frac{1}{1+e^{-L}}\\)（sigmoid）

## 损失函数（度量模型性能的方法）

### 均方误差，mean squared error
![mqe](http://zihuaweng.github.io/post_images/loss/mqe.png)


### 交叉熵
首先介绍一下熵。一条消息的信息量等于他的不确定的多少。
假设要从32个足球队伍里面选择1个冠军队伍，我们可以猜5次就能决定是那一支。所以这个信息量为5（log32=5）。
但是有时候我们已经知道了不是每支队伍的取胜概率都一样，我们可以将厉害的几只队伍放在一起，剩下的直接排除，那么信息量就应该是\\( H=-(p1\*logp1+p2\*logp2+...+p32\*logp32)\\)
所以信息熵的定义为\\[ H(x)=-\sum(p(x)\*log(p(x)))\\]


可惜的是,均方误差和平均绝对误差在使用基于梯度的优化方法时往往成效不
佳。一些饱和的输出单元当结合这些代价函数时会产生非常小的梯度。这就是为什
么交叉熵代价函数比均方误差或者平均绝对误差更受欢迎的原因之一了,即使是在
没必要估计整个 p(y | x) 分布时。
交叉熵可在神经网络(机器学习)中作为损失函数，p表示真实标记的分布，q则为训练后的模型的预测标记分布，交叉熵损失函数可以衡量p与q的相似性。交叉熵作为损失函数还有一个好处是使用sigmoid函数在梯度下降时能避免均方误差损失函数学习速率降低的问题，因为学习速率可以被输出的误差所控制。

例子1：

computed       | targets              | correct?
-----------------------------------------------
0.3  0.3  0.4  | 0  0  1 (democrat)   | yes
0.3  0.4  0.3  | 0  1  0 (republican) | yes
0.1  0.2  0.7  | 1  0  0 (other)      | 

例子2：
computed       | targets              | correct?
-----------------------------------------------
0.1  0.2  0.7  | 0  0  1 (democrat)   | yes
0.1  0.7  0.2  | 0  1  0 (republican) | yes
0.3  0.4  0.3  | 1  0  0 (other)      | no


tensorflow中有几种方式计算损失：

sigmoid系列：
- tf.nn.sigmoid_cross_entropy_with_logits
- tf.nn.weighted_cross_entropy_with_logits
- tf.losses.sigmoid_cross_entropy

sigmoid通常只在二分类上使用，但是tensorflow中，如果各个类别相互独立，sigmoid函数可用于多分类（tf.nn.sigmoid_cross_entropy_with_logits）。
label可以是one-hot encoded或者soft class probabilites(概率比值)
tf.nn.weighted_cross_entropy_with_logits
tf.losses.sigmoid_cross_entropy 两个函数可以设置权重，这样有助于训练不平衡数据集。

softmax系列：
- tf.nn.softmax_cross_entropy_with_logits
- tf.losses.softmax_cross_entropy

适用于类别>=2的分类问题。label可以是one-hot encoded或者soft class probabilites(概率比值)。
tf.losses.sigmoid_cross_entropy同样可以添加权重。

spare系列：
- tf.nn.sparse_softmax_cross_entropy_with_logits
- tf.losses.sparse_softmax_cross_entropy

适用于类别>=2的分类问题。与softmax系列唯一的不同是label是class index的int，不是上面的one-hot encoded。tf.losses.sparse_softmax_cross_entropy同样可以设置权重。



## Reference:
1. https://stackoverflow.com/questions/47034888/how-to-choose-cross-entropy-loss-in-tensorflow
2. 数学之美
3. https://jamesmccaffrey.wordpress.com/2013/11/05/why-you-should-use-cross-entropy-error-instead-of-classification-error-or-mean-squared-error-for-neural-network-classifier-training/
3. 


