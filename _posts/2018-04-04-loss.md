---
layout:     post
title:      损失函数汇总及tensorflow实践
subtitle:   
date:       2018-04-01
author:     zihuaweng
header-img: img/post_bg_region.png
catalog: true
tags:
    - loss function
    - tensorflow
---

## 损失函数

sigmoid和softmax函数的存在是将计算的logit转化为概率p。

概率p的logit是L=\ln\frac{p}{1-p}。其中\frac{p}{1-p}是odd(两者比率)，logit反过来就是p= \frac{1}{1+e^{-L}}（sigmoid）

tensorflow中有几种方式计算损失：

sigmoid系列：
- tf.nn.sigmoid_cross_entropy_with_logits
- tf.nn.weighted_cross_entropy_with_logits
- tf.losses.sigmoid_cross_entropy

sigmoid通常只在二分类上使用，但是tensorflow中，如果各个类别相互独立，sigmoid函数可用于多分类（tf.nn.sigmoid_cross_entropy_with_logits）。
label可以是one-hot encoded或者soft class probabilites(概率比值)
tf.nn.weighted_cross_entropy_with_logits
tf.losses.sigmoid_cross_entropy 两个函数可以设置权重，这样有助于训练不平衡数据集。

softmax系列：
- tf.nn.softmax_cross_entropy_with_logits
- tf.losses.softmax_cross_entropy

适用于类别>=2的分类问题。label可以是one-hot encoded或者soft class probabilites(概率比值)。
tf.losses.sigmoid_cross_entropy同样可以添加权重。

spare系列：
- tf.nn.sparse_softmax_cross_entropy_with_logits
- tf.losses.sparse_softmax_cross_entropy

适用于类别>=2的分类问题。与softmax系列唯一的不同是label是class index的int，不是上面的one-hot encoded。tf.losses.sparse_softmax_cross_entropy同样可以设置权重。



## Reference:
1. https://stackoverflow.com/questions/47034888/how-to-choose-cross-entropy-loss-in-tensorflow

