---
layout:     post
title:      损失函数汇总及tensorflow实践
subtitle:   
date:       2018-04-01
author:     zihuaweng
header-img: img/post_header.jpg
catalog: true
tags:
    - loss function
    - tensorflow
---

## 激活函数
sigmoid和softmax函数的存在是将计算的logit转化为概率p。sigmoid用于二分类逻辑回归，softmax用于多分类逻辑回归。

### sigmoid
![sigmoid](http://zihuaweng.github.io/post_images/loss/sigmoid.png)
求导：
![sigmoid-dao](http://zihuaweng.github.io/post_images/loss/sigmoid-dao.png)

概率p的logit是\\(L=\ln\frac{p}{1-p} \\)。其中\\(\frac{p}{1-p}\\)是odd(两者比率)，logit反过来就是\\(p= \frac{1}{1+e^{-L}}\\)（sigmoid）

### softmax
![softmax](http://zihuaweng.github.io/post_images/loss/softmax.png)
可以看出，当二分类问题使用softmax，softmax公式就等于sigmoid公式。
![duibi](http://zihuaweng.github.io/post_images/loss/duibi.png)

一个计算小技巧：
因为都是指数函数，所以分子和分母很有可能非常大，通过下面的转换，其中logC = -max(f)
![youhua](http://zihuaweng.github.io/post_images/loss/youhua.png)
~~~python
f = np.array([123, 456, 789]) # example with 3 classes and each having large scores
p = np.exp(f) / np.sum(np.exp(f)) # Bad: Numeric problem, potential blowup

# instead: first shift the values of f so that the highest number is 0:
f -= np.max(f) # f becomes [-666, -333, 0]
p = np.exp(f) / np.sum(np.exp(f)) # safe to do, gives the correct answer
~~~

## 损失函数（度量模型性能的方法）

### 均方误差，mean squared error
![mqe](http://zihuaweng.github.io/post_images/loss/mqe.png)


### 交叉熵
首先介绍一下熵。一条消息的信息量等于他的不确定的多少。
假设要从32个足球队伍里面选择1个冠军队伍，我们可以猜5次就能决定是那一支。所以这个信息量为5（log32=5）。
但是有时候我们已经知道了不是每支队伍的取胜概率都一样，我们可以将厉害的几只队伍放在一起，剩下的直接排除，那么信息量就应该是\\( H=-(p1\*logp1+p2\*logp2+...+p32\*logp32)\\)
所以信息熵的定义为\\[ H(x)=-\sum(p(x)\*log(p(x)))\\]

真实分布p与估计分布q之间的交叉熵（相对熵）定义为：
![cross-entropy-p-q](http://zihuaweng.github.io/post_images/loss/cross-entropy-p-q.png)
相对熵越大，两个函数差异越大，相对熵越小，两个函数差异越小。对于概率分布或者概率密度函数，如果取值均大于零，相对熵可以度量两个随机分布的差异性。

所以评价一个模型，可以判断预测结果分布与真实结果分布的相对熵是否足够低。

### 比较
分类器和损失函数选择汇总
![loss_activation](http://zihuaweng.github.io/post_images/loss/loss_activation.png)

均方误差在使用基于梯度的优化方法时往往成效不佳。交叉熵作为损失函数还有一个好处是使用sigmoid函数之类的饱和输出单元时在梯度下降时能避免均方误差损失函数学习速率降低的问题，因为学习速率可以被输出的误差所控制。

## tensorflow中有几种方式计算损失函数的方式：

sigmoid系列：
- tf.nn.sigmoid_cross_entropy_with_logits
- tf.nn.weighted_cross_entropy_with_logits
- tf.losses.sigmoid_cross_entropy

sigmoid通常只在二分类上使用，但是tensorflow中，如果各个类别相互独立，sigmoid函数可用于多分类（tf.nn.sigmoid_cross_entropy_with_logits）。
label可以是one-hot encoded或者soft class probabilites(概率比值)
tf.nn.weighted_cross_entropy_with_logits
tf.losses.sigmoid_cross_entropy 两个函数可以设置权重，这样有助于训练不平衡数据集。

softmax系列：
- tf.nn.softmax_cross_entropy_with_logits
- tf.losses.softmax_cross_entropy

适用于类别>=2的分类问题。label可以是one-hot encoded或者soft class probabilites(概率比值)。
tf.losses.sigmoid_cross_entropy同样可以添加权重。

spare系列：
- tf.nn.sparse_softmax_cross_entropy_with_logits
- tf.losses.sparse_softmax_cross_entropy

适用于类别>=2的分类问题。与softmax系列唯一的不同是label是class index的int，不是上面的one-hot encoded。tf.losses.sparse_softmax_cross_entropy同样可以设置权重。



## Reference:
1. https://stackoverflow.com/questions/47034888/how-to-choose-cross-entropy-loss-in-tensorflow
2. 数学之美
3. 深度学习
3. https://jamesmccaffrey.wordpress.com/2013/11/05/why-you-should-use-cross-entropy-error-instead-of-classification-error-or-mean-squared-error-for-neural-network-classifier-training/
3. http://cs231n.github.io/
https://www.zhihu.com/question/41252833
https://www.zhihu.com/question/65288314/answer/244588905



